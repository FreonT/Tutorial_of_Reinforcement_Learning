{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初回インストール物"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.5.1+cpu in c:\\users\\freon\\anaconda3\\lib\\site-packages (1.5.1+cpu)\n",
      "Requirement already satisfied: torchvision==0.6.1+cpu in c:\\users\\freon\\anaconda3\\lib\\site-packages (0.6.1+cpu)\n",
      "Requirement already satisfied: future in c:\\users\\freon\\anaconda3\\lib\\site-packages (from torch==1.5.1+cpu) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\freon\\anaconda3\\lib\\site-packages (from torch==1.5.1+cpu) (1.18.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\freon\\anaconda3\\lib\\site-packages (from torchvision==0.6.1+cpu) (7.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "display_notebook = False # サーバ上でやる場合はTrueにすること（フレームレート下がるので不要ならFalse）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAC.model import SACNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAC.learn import calc_critic_loss, calc_policy_loss, update_params, calc_entropy_loss, update_params, soft_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(learner_model, optim,\n",
    "           observation_buffer,\n",
    "           action_buffer,\n",
    "           reward_buffer,\n",
    "           done_buffer,\n",
    "           gamma=0.99,\n",
    "           weights=1,\n",
    "           tau=0.005):\n",
    "    soft_update(learner_model.critic_target, learner_model.critic, tau)\n",
    "    observations = torch.Tensor(observation_buffer)\n",
    "    states = observations[1:]\n",
    "    next_states = observations[:-1]\n",
    "    actions = torch.Tensor(action_buffer)\n",
    "    rewards = torch.Tensor(reward_buffer)\n",
    "    dones = torch.Tensor(done_buffer)\n",
    "    \n",
    "    q1_loss, q2_loss, errors, mean_q1, mean_q2 =\\\n",
    "        calc_critic_loss(learner_model, states, actions, rewards, dones, next_states, weights, optim.alpha, gamma)\n",
    "    policy_loss, entropies = calc_policy_loss(learner_model, states, weights, optim.alpha)\n",
    "\n",
    "    update_params(\n",
    "        optim.q1_optim, learner_model.critic.Q1, q1_loss, 40)\n",
    "    update_params(\n",
    "        optim.q2_optim, learner_model.critic.Q2, q2_loss, 40)\n",
    "    update_params(\n",
    "        optim.policy_optim, learner_model.policy, policy_loss, 40)\n",
    "\n",
    "    if optim.entropy_tuning:\n",
    "        entropy_loss = calc_entropy_loss(optim.log_alpha, optim.target_entropy, entropies, weights)\n",
    "        update_params(optim.alpha_optim, None, entropy_loss)\n",
    "        optim.alpha = optim.log_alpha.exp()\n",
    "\n",
    "    #actor_model.policy.load_state_dict(learner_model.policy.state_dict())\n",
    "    #episode_returns = batch[\"episode_return\"][batch[\"done\"]]\n",
    "    stats = {\n",
    "    #    \"0_episode_returns\": tuple(episode_returns.cpu().numpy()),\n",
    "    #    \"1_mean_episode_return\": torch.mean(episode_returns).item(),\n",
    "        \"2_q1_loss\": q1_loss.item(),\n",
    "        \"3_q2_loss\": q2_loss.item(),\n",
    "        \"4_policy_loss\": policy_loss.item(),\n",
    "        \"5_entropy_loss\": entropy_loss.item(),\n",
    "        \"6_alpha\": optim.alpha.item(),\n",
    "        \"7_entoropy\": entropies.mean().item()\n",
    "    }\n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer:\n",
    "    def __init__(self, model, entropy_tuning=True, learning_rate=0.01, device=\"cpu\"):\n",
    "        self.policy_optim = Adam(model.policy.parameters(), lr=learning_rate)\n",
    "        self.q1_optim = Adam(model.critic.Q1.parameters(), lr=learning_rate)\n",
    "        self.q2_optim = Adam(model.critic.Q2.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.entropy_tuning = entropy_tuning\n",
    "        self.device = device\n",
    "\n",
    "        if self.entropy_tuning:\n",
    "            # Target entropy is -|A|.\n",
    "            self.target_entropy = -torch.prod(torch.Tensor(\n",
    "                model.num_actions).to(device)).item()\n",
    "            # We optimize log(alpha), instead of alpha.\n",
    "            self.log_alpha = torch.zeros(\n",
    "                1, requires_grad=True, device=device)\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "            self.alpha_optim = Adam([self.log_alpha], lr=learning_rate)\n",
    "        else:\n",
    "            # fixed alpha\n",
    "            self.alpha = torch.tensor(ent_coef).to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optim, n_train_steps, env_name, tau=0.005, seed=None):\n",
    "    start_time = time.time()\n",
    "    env = gym.make(env_name)\n",
    "    if seed != None:\n",
    "        env.seed(seed)\n",
    "    \n",
    "    t = 0\n",
    "    while(True):\n",
    "        observation_buffer = []\n",
    "        action_buffer = []\n",
    "        reward_buffer = []\n",
    "        done_buffer = []\n",
    "        \n",
    "        observation = env.reset()\n",
    "        observation_buffer.append(observation)\n",
    "        \n",
    "        total_reward = 0\n",
    "        while(True):\n",
    "            agent_output, _ = model.act(observation)\n",
    "            action = agent_output[\"action\"].item()\n",
    "            \n",
    "            observation, reward, done, info = env.step(action) # 行動を環境に反映させる\n",
    "            total_reward += reward\n",
    "            \n",
    "            observation_buffer.append(observation)\n",
    "            reward_buffer.append(reward)\n",
    "            action_buffer.append(action)\n",
    "\n",
    "            if done:\n",
    "                done_buffer.append(1)\n",
    "            else:\n",
    "                done_buffer.append(0)\n",
    "            t += 1\n",
    "            if done:\n",
    "                display.clear_output(wait=True)\n",
    "                stats = update(model, optim, observation_buffer, action_buffer, reward_buffer, done_buffer, gamma=0.99, tau=tau)\n",
    "                \n",
    "                print(\"step : \"+str(t)+\" / \"+str(n_train_steps))\n",
    "                print(\"sps : \"+str(t/(time.time()-start_time)))\n",
    "                print(\"total reward : \"+str(total_reward))\n",
    "                print(\"Stats:\\n%s\"%( pprint.pformat(stats) ) )\n",
    "                break\n",
    "        if t >= n_train_steps:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, env_name, n_episode=1, seed=None):\n",
    "    env = gym.make(env_name)\n",
    "    if seed != None:\n",
    "        env.seed(seed)\n",
    "    \n",
    "    sum_total_reward = 0\n",
    "    for ep in range(n_episode):\n",
    "        observation = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while(True):\n",
    "            img = env.render(\"rgb_array\") # 画面の表示\n",
    "            if display_notebook:\n",
    "                plt.imshow()\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(plt.gcf())\n",
    "\n",
    "            action = model.act_greedy(observation)\n",
    "            observation, reward, done, info = env.step(action.item()) # 行動を環境に反映させる\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print(\"total reward : \"+str(total_reward))\n",
    "                sum_total_reward += total_reward\n",
    "                break\n",
    "    print(\"average reward : \"+str(sum_total_reward/n_episode))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 20379 / 20000\n",
      "sps : 2029.0943494442502\n",
      "total reward : -500.0\n",
      "Stats:\n",
      "{'2_q1_loss': 0.03711812570691109,\n",
      " '3_q2_loss': 0.027317799627780914,\n",
      " '4_policy_loss': -0.11278628557920456,\n",
      " '5_entropy_loss': -0.13494110107421875,\n",
      " '6_alpha': 0.8813081383705139,\n",
      " '7_entoropy': 1.0941256284713745}\n",
      "\n",
      "test\n",
      "total reward : -88.0\n",
      "total reward : -87.0\n",
      "total reward : -112.0\n",
      "average reward : -95.66666666666667\n",
      "\n",
      "hyper parameters\n",
      "lr : 0.003\n",
      "hidden units : [128, 128]\n",
      "tau : 0.05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name = \"CartPole-v0\" # 扱う環境の指定\n",
    "#env_name = 'MountainCar-v0'\n",
    "env_name = \"Acrobot-v1\"\n",
    "\n",
    "if env_name == 'MountainCar-v0':\n",
    "    n_train_steps = 50_000\n",
    "    lr=0.001\n",
    "    hidden_units=[256, 256]\n",
    "    tau=0.02\n",
    "\n",
    "if env_name == \"Acrobot-v1\":\n",
    "    n_train_steps = 20_000\n",
    "    lr=0.003\n",
    "    hidden_units=[128, 128]\n",
    "    tau=0.05\n",
    "if env_name == \"CartPole-v0\":\n",
    "    n_train_steps = 8_000\n",
    "    lr=0.001\n",
    "    hidden_units=[128, 128]\n",
    "    tau=0.02\n",
    "\n",
    "# parameter setting\n",
    "env = gym.make(env_name)\n",
    "observation_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "env.close()\n",
    "\n",
    "# seed setting\n",
    "seed = 0\n",
    "torch.manual_seed(seed) \n",
    "#random.seed(seed)  \n",
    "np.random.seed(seed)  \n",
    "\n",
    "\n",
    "learner_model = SACNet(observation_shape=observation_shape, num_actions=num_actions)\n",
    "optim = optimizer(learner_model, learning_rate=lr)\n",
    "\n",
    "print(\"train\")\n",
    "train(learner_model, optim, n_train_steps, env_name, tau=tau, seed=seed)\n",
    "\n",
    "print(\"\\ntest\")\n",
    "# Greedyな行動のみでテスト\n",
    "test(learner_model, env_name, n_episode=3, seed=seed)\n",
    "\n",
    "print(\"\\nhyper parameters\")\n",
    "print(\"lr : \"+str(lr))\n",
    "print(\"hidden units : \"+str(hidden_units))\n",
    "print(\"tau : \"+str(tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みモデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"./save_data/\" + env_name\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "checkpointpath = os.path.expandvars(\n",
    "    os.path.expanduser(folder+\"/model.tar\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        #\"model_state_dict\": model.state_dict(),\n",
    "        \"learner_model_state_dict\": learner_model.state_dict(),\n",
    "        \"optimizer_state_dict\": optim.state_dict(),\n",
    "    },\n",
    "    checkpointpath,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
