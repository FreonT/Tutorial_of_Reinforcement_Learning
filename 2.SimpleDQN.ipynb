{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初回インストール物"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.5.1+cpu in c:\\users\\freon\\anaconda3\\lib\\site-packages (1.5.1+cpu)\n",
      "Requirement already satisfied: torchvision==0.6.1+cpu in c:\\users\\freon\\anaconda3\\lib\\site-packages (0.6.1+cpu)\n",
      "Requirement already satisfied: future in c:\\users\\freon\\anaconda3\\lib\\site-packages (from torch==1.5.1+cpu) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\freon\\anaconda3\\lib\\site-packages (from torch==1.5.1+cpu) (1.18.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\freon\\anaconda3\\lib\\site-packages (from torchvision==0.6.1+cpu) (7.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "display_notebook = False # サーバ上でやる場合はTrueにすること（フレームレート下がるので不要ならFalse）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_xavier(m):\n",
    "    if isinstance(m, nn.Linear)\\\n",
    "            or isinstance(m, nn.Conv2d)\\\n",
    "            or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def create_linear_network(input_dim, output_dim, hidden_units=[256, 256],\n",
    "                          hidden_activation=nn.ReLU(), output_activation=None,\n",
    "                          initializer=weights_init_xavier):\n",
    "    model = []\n",
    "    units = input_dim\n",
    "    for next_units in hidden_units:\n",
    "        model.append(nn.Linear(units, next_units, bias=False))\n",
    "        model.append(hidden_activation)\n",
    "        units = next_units\n",
    "\n",
    "    model.append(nn.Linear(units, output_dim))\n",
    "    if output_activation is not None:\n",
    "        model.append(output_activation)\n",
    "\n",
    "    return nn.Sequential(*model).apply(initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_units=[256, 256], initializer=weights_init_xavier):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.Q = create_linear_network(\n",
    "            num_inputs, num_actions, hidden_units=hidden_units,\n",
    "            initializer=initializer)\n",
    "        \n",
    "\n",
    "    def forward(self, states):\n",
    "        states = torch.Tensor(states)\n",
    "        q = self.Q(states)\n",
    "        \n",
    "        return q\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, epsilon_start=0.3, epsilon_end=0.1, learning_rate = 0.001, hidden_units=[256, 256], tau=0.2):\n",
    "        env = gym.make(env_name)\n",
    "        num_inputs = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.model = QNetwork(num_inputs, self.num_actions, hidden_units=hidden_units)\n",
    "        self.learner_model = QNetwork(num_inputs, self.num_actions, hidden_units=hidden_units)\n",
    "        hard_update(self.model, self.learner_model)\n",
    "        self.optim = Adam(self.learner_model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.a = (epsilon_end - epsilon_start)\n",
    "        self.b = epsilon_start\n",
    "        \n",
    "    def get_action_greedy(self, observation):\n",
    "        q = self.model(observation).detach().numpy()\n",
    "        action = np.argmax(q)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def get_action(self, observation, step):\n",
    "        epsilon = self.a*step + self.b\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.get_action_greedy(observation)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for t, s in zip(target.parameters(), source.parameters()):\n",
    "        t.data.copy_(t.data * (1.0 - tau) + s.data * tau)\n",
    "        \n",
    "def hard_update(target, source):\n",
    "    target.load_state_dict(source.state_dict())\n",
    "        \n",
    "def update_params(optim, network, loss, grad_clip=40, retain_graph=False):\n",
    "    optim.zero_grad()\n",
    "    loss.backward(retain_graph=retain_graph)\n",
    "    if grad_clip is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(network.parameters(), grad_clip)\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_current_q(model, states, actions):\n",
    "    curr_q = model(states)\n",
    "    curr_q = curr_q.gather(1, actions.unsqueeze(1).long())\n",
    "    \n",
    "    return curr_q\n",
    "\n",
    "def calc_target_q(model, rewards, next_states, dones, gamma_n=0.99):\n",
    "    with torch.no_grad():\n",
    "        next_q = model(next_states)\n",
    "        #next_v = next_q.max(dim=1, keepdim=True).value()\n",
    "        next_v, index = torch.max(next_q, axis=1, keepdim=True)\n",
    "    \n",
    "    target_q = rewards.view_as(next_v) + (1.0 - dones.view_as(next_v)) * gamma_n * next_v\n",
    "    \n",
    "    \n",
    "    return target_q\n",
    "    \n",
    "def calc_critic_loss(model, states, actions, rewards, dones, next_states, gamma):\n",
    "    curr_q = calc_current_q(model, states, actions)\n",
    "    target_q = calc_target_q(model, rewards, next_states, dones, gamma_n=gamma)\n",
    "\n",
    "    # TD errors for updating priority weights\n",
    "    errors = torch.abs(curr_q.detach() - target_q)\n",
    "    # We log means of Q to monitor training.\n",
    "    mean_q = curr_q.detach().mean().item()\n",
    "    \n",
    "    # Critic loss is mean squared TD errors with priority weights.\n",
    "    q_loss = torch.mean((curr_q - target_q).pow(2))\n",
    "\n",
    "    return q_loss, errors, mean_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, observation_buffer, action_buffer, reward_buffer, done_buffer, gamma=0.99):\n",
    "    observations = torch.Tensor(observation_buffer)\n",
    "    states = observations[1:]\n",
    "    next_states = observations[:-1]\n",
    "    actions = torch.Tensor(action_buffer)\n",
    "    rewards = torch.Tensor(reward_buffer)\n",
    "    dones = torch.Tensor(done_buffer)\n",
    "    \n",
    "    q_loss, errors, mean_q = calc_critic_loss(agent.learner_model.Q, states, actions, rewards, dones, next_states, gamma)\n",
    "    \n",
    "    update_params(agent.optim, agent.learner_model, q_loss)\n",
    "    \n",
    "    return q_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(agent, n_train_steps, env_name, tau=0.005, seed=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    if seed != None:\n",
    "        env.seed(seed)\n",
    "    \n",
    "    last_soft_update = 0\n",
    "    t = 0\n",
    "    while(True):\n",
    "        observation_buffer = []\n",
    "        action_buffer = []\n",
    "        reward_buffer = []\n",
    "        done_buffer = []\n",
    "        observation = env.reset()\n",
    "        observation_buffer.append(observation)\n",
    "        \n",
    "        total_reward = 0\n",
    "        while(True):\n",
    "            \n",
    "            action = agent.get_action(observation, t/n_train_steps)\n",
    "            observation, reward, done, info = env.step(action) # 行動を環境に反映させる\n",
    "            total_reward += reward\n",
    "            \n",
    "            observation_buffer.append(observation)\n",
    "            reward_buffer.append(reward)\n",
    "            action_buffer.append(action)\n",
    "            if done:\n",
    "                done_buffer.append(1)\n",
    "            else:\n",
    "                done_buffer.append(0)\n",
    "            t += 1\n",
    "            if done:\n",
    "                #print(\"done\")\n",
    "                display.clear_output(wait=True)\n",
    "                q_loss = update(agent, observation_buffer, action_buffer, reward_buffer, done_buffer, gamma=0.99)\n",
    "                \n",
    "                soft_update(agent.model, agent.learner_model, tau)\n",
    "                \n",
    "                print(\"step : \"+str(t)+\" / \"+str(n_train_steps))\n",
    "                print(\"sps : \"+str(t/(time.time()-start_time)))\n",
    "                print(\"total reward : \"+str(total_reward))\n",
    "                print(\"loss : \"+str(q_loss.item()))\n",
    "                # エピソードが終了したら、環境をリセットする\n",
    "                break\n",
    "        if t >= n_train_steps:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, env_name, n_episode=1, seed=None):\n",
    "    env = gym.make(env_name)\n",
    "    if seed != None:\n",
    "        env.seed(seed)\n",
    "    \n",
    "    sum_total_reward = 0\n",
    "    for ep in range(n_episode):\n",
    "        observation = env.reset()\n",
    "        total_reward = 0\n",
    "        while(True):\n",
    "            img = env.render(\"rgb_array\") # 画面の表示\n",
    "            if display_notebook:\n",
    "                plt.imshow()\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(plt.gcf())\n",
    "\n",
    "            action = model.get_action_greedy(observation) # ランダムな行動をとる\n",
    "            observation, reward, done, info = env.step(action) # 行動を環境に反映させる\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print(\"total reward : \"+str(total_reward))\n",
    "                sum_total_reward += total_reward\n",
    "                break\n",
    "    print(\"\")\n",
    "    print(\"average reward : \"+str(sum_total_reward/n_episode))\n",
    "    env.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 20031 / 20000\n",
      "sps : 2643.4876217718074\n",
      "total reward : -108.0\n",
      "loss : 2.7201170921325684\n",
      "\n",
      "test\n",
      "total reward : -127.0\n",
      "\n",
      "average reward : -127.0\n"
     ]
    }
   ],
   "source": [
    "#env_name = 'MountainCar-v0'\n",
    "\n",
    "#env_name = \"CartPole-v0\" # 扱う環境の指定\n",
    "env_name = \"Acrobot-v1\"\n",
    "\n",
    "if env_name == 'MountainCar-v0':\n",
    "    # 単純なDQNではうまく学習できず\n",
    "    n_train_steps = 100_000\n",
    "    epsilon_start = 1\n",
    "    epsilon_end = 0.01\n",
    "    lr=0.001\n",
    "    hidden_units=[24, 48]\n",
    "    tau=0.1\n",
    "\n",
    "if env_name == \"Acrobot-v1\":\n",
    "    n_train_steps = 20_000\n",
    "    epsilon_start = 0.3\n",
    "    epsilon_end = 0.01\n",
    "    lr=0.003\n",
    "    hidden_units=[128, 128]\n",
    "    tau=0.05\n",
    "if env_name == \"CartPole-v0\":\n",
    "    n_train_steps = 8_000\n",
    "    epsilon_start = 0.2\n",
    "    epsilon_end = 0.01\n",
    "    lr=0.001\n",
    "    hidden_units=[128, 128]\n",
    "    tau=0.05\n",
    "\n",
    "seed=0\n",
    "torch.manual_seed(seed) \n",
    "np.random.seed(seed)  \n",
    "\n",
    "model = Agent(epsilon_start=epsilon_start, epsilon_end=epsilon_end, learning_rate=lr, hidden_units=hidden_units)\n",
    "\n",
    "train(model, n_train_steps, env_name, tau=tau, seed=seed)\n",
    "\n",
    "print(\"\\ntest\")\n",
    "# Greedyな行動のみでテスト\n",
    "test(model, env_name,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward : -93.0\n",
      "total reward : -124.0\n",
      "total reward : -117.0\n",
      "\n",
      "average reward : -111.33333333333333\n"
     ]
    }
   ],
   "source": [
    "test(model, env_name,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みモデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"./save_data/\" + env_name\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "checkpointpath = os.path.expandvars(\n",
    "    os.path.expanduser(folder+\"/model.tar\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": agenta.model.state_dict(),\n",
    "        \"learner_model_state_dict\": agenta.learner_model.state_dict(),\n",
    "        \"optimizer_state_dict\": agenta.optim.state_dict(),\n",
    "    },\n",
    "    checkpointpath,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
